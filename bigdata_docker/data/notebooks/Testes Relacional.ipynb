{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, dataframe\n",
    "from pyspark.sql.functions import when, col, sum, count, isnan, round\n",
    "from pyspark.sql.functions import regexp_replace, concat_ws, sha2, rtrim, substring\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_date\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import year, month, dayofmonth, quarter\n",
    "from pyspark.sql.types import DecimalType\n",
    "from pyspark.sql.functions import trim, regexp_replace, when, col\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar tabelas endereco e remover a primeira linha do cabeçalho\n",
    "\n",
    "df_endereco = spark.read.table(\"desafio_curso.endereco\")\n",
    "rdd = df_endereco.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_endereco = rdd.toDF(df_endereco.schema)\n",
    "\n",
    "df_clientes = spark.read.table(\"desafio_curso.clientes\")\n",
    "rdd = df_clientes.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_clientes = rdd.toDF(df_clientes.schema)\n",
    "\n",
    "df_divisao = spark.read.table(\"desafio_curso.divisao\")\n",
    "rdd = df_divisao.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_divisao = rdd.toDF(df_divisao.schema)\n",
    "\n",
    "df_regiao = spark.read.table(\"desafio_curso.regiao\")\n",
    "rdd = df_regiao.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_regiao = rdd.toDF(df_regiao.schema)\n",
    "\n",
    "df_vendas = spark.read.table(\"desafio_curso.vendas\")\n",
    "rdd = df_vendas.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_vendas = rdd.toDF(df_vendas.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de valor de vendas:\n",
      "+--------------------+\n",
      "|total_vendas_decimal|\n",
      "+--------------------+\n",
      "|        186186769.05|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 - soma de sales_amount\n",
    "print(\"Total de valor de vendas:\")\n",
    "df_vendas = df_vendas.withColumn(\"sales_amount\", regexp_replace(\"sales_amount\", \",\", \".\"))\n",
    "resultado = df_vendas.agg({\"sales_amount\": \"sum\"}).withColumnRenamed(\"sum(sales_amount)\", \"total_vendas\")\n",
    "resultado_decimal = resultado.select(resultado[\"total_vendas\"].cast(DecimalType(18, 2)).alias(\"total_vendas_decimal\"))\n",
    "resultado_decimal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total produtos vendidos:\n",
      "+------------+\n",
      "|total_vendas|\n",
      "+------------+\n",
      "|   2943194.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 - soma de sales_quantity\n",
    "print(\"Total produtos vendidos:\")\n",
    "total_vendas = df_vendas.agg(sum('sales_quantity').alias('total_vendas'))\n",
    "total_vendas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produto mais vendido:\n",
      "+--------------------+----------------+\n",
      "|                item|quantidade_total|\n",
      "+--------------------+----------------+\n",
      "|Better Large Cann...|        590343.0|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - produto mais vendido\n",
    "print(\"Produto mais vendido:\")\n",
    "agrupado_por_item = df_vendas.groupBy('item').agg(sum('sales_quantity').alias('quantidade_total'))\n",
    "produto_mais_vendido = agrupado_por_item.orderBy('quantidade_total', ascending=False).limit(1)\n",
    "produto_mais_vendido.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Produtos mais vendidos:\n",
      "+--------------------+----------------+\n",
      "|                item|quantidade_total|\n",
      "+--------------------+----------------+\n",
      "|Better Large Cann...|        590343.0|\n",
      "|High Top Dried Mu...|        377259.0|\n",
      "|Better Canned Tun...|        266996.0|\n",
      "|   Walrus Chardonnay|        212022.0|\n",
      "|Red Spade Pimento...|        163296.0|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 - 5 produtos mais sales_quantity\n",
    "print(\"5 Produtos mais vendidos:\")\n",
    "resultado = df_vendas.groupBy('item').agg(sum('sales_quantity').alias('quantidade_total')) \\\n",
    "            .orderBy('quantidade_total', ascending=False) \\\n",
    "            .limit(5)\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Produtos com maior valor de venda:\n",
      "+--------------------+------------------+\n",
      "|                item|valor_total_vendas|\n",
      "+--------------------+------------------+\n",
      "|Better Large Cann...|       15454172.47|\n",
      "|High Top Dried Mu...|       13368414.53|\n",
      "|Red Spade Pimento...|        5711486.45|\n",
      "|Better Canned Tun...|        5693075.12|\n",
      "|        Ebony Squash|        5380727.75|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 - 5 produtos mais sales_amount\n",
    "print(\"5 Produtos com maior valor de venda:\")\n",
    "resultado = df_vendas.groupBy('item').agg(sum('sales_amount').alias('valor_total_vendas')) \\\n",
    "            .orderBy('valor_total_vendas', ascending=False) \\\n",
    "            .limit(5) \\\n",
    "            .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\n",
    "\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de venda e quantidade de vendas por mês:\n",
      "+-----+----------------+------------------+\n",
      "|month|quantidade_total|valor_total_vendas|\n",
      "+-----+----------------+------------------+\n",
      "| null|            null|              null|\n",
      "|   01|        325560.0|       19471739.54|\n",
      "|   02|        327750.0|       20497349.91|\n",
      "|   03|        328798.0|       21714172.68|\n",
      "|   04|        193063.0|       12112134.49|\n",
      "|   05|        202787.0|       11053298.15|\n",
      "|   06|        264470.0|       15852396.38|\n",
      "|   07|        196932.0|       13287585.39|\n",
      "|   08|        242825.0|       14590611.40|\n",
      "|   09|        241863.0|       16466268.87|\n",
      "|   10|        184381.0|       12829983.51|\n",
      "|   11|        221260.0|       13794762.06|\n",
      "|   12|        213505.0|       14516466.67|\n",
      "+-----+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 - sales_quantity por mes e  sales_amount por mes\n",
    "print(\"Valor de venda e quantidade de vendas por mês:\")\n",
    "\n",
    "from pyspark.sql.functions import col, sum, to_date, date_format\n",
    "\n",
    "df_vendas = df_vendas.withColumn(\"month\", date_format(to_date(col(\"invoice_date\"), \"dd/MM/yyyy\"), \"MM\"))\n",
    "vendas_por_mes = df_vendas.groupBy(\"month\")\\\n",
    "                    .agg(sum(\"sales_quantity\").alias(\"quantidade_total\"), sum(\"sales_amount\").alias(\"valor_total_vendas\"))\\\n",
    "                    .orderBy('month')\\\n",
    "                    .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\n",
    "\n",
    "vendas_por_mes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de venda em moeda e em porcentagem por país:\n",
      "+-------+------------------+-----------+\n",
      "|country|valor_total_vendas|porcentagem|\n",
      "+-------+------------------+-----------+\n",
      "|     US|      100312451.37|      53.88|\n",
      "|     UK|       15555235.25|       8.35|\n",
      "|     AU|       10962238.21|       5.89|\n",
      "|     CA|        6320257.36|       3.39|\n",
      "|     IR|        2109229.10|       1.13|\n",
      "+-------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 - valor de venda por country - mostrar porcentagem\n",
    "print(\"Valor de venda em moeda e em porcentagem por país:\")\n",
    "\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "df_vendas_pais = df_vendas.join(df_endereco, df_vendas.customer_key == df_endereco.address_number)\\\n",
    "                         .groupBy(\"country\")\\\n",
    "                         .agg(sum(\"sales_amount\").alias(\"valor_total_vendas\"))\\\n",
    "                         .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\n",
    "\n",
    "total_vendas = df_vendas.agg(sum(\"sales_amount\")).collect()[0][0]\n",
    "df_vendas_pais = df_vendas_pais.withColumn(\"porcentagem\", col(\"valor_total_vendas\") / total_vendas * 100)\\\n",
    "                    .orderBy('porcentagem', ascending=False)\\\n",
    "                    .withColumn('porcentagem', col('porcentagem').cast(DecimalType(18, 2)))\n",
    "\n",
    "df_vendas_pais.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de venda por ano:\n",
      "+----+------------------+\n",
      "|year|valor_total_vendas|\n",
      "+----+------------------+\n",
      "|null|              null|\n",
      "|2017|       77906591.65|\n",
      "|2018|       87462706.40|\n",
      "|2019|       20817471.00|\n",
      "+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 - sales_amount por ano\n",
    "print(\"Valor de venda por ano:\")\n",
    "\n",
    "df_vendas = df_vendas.withColumn(\"year\", date_format(to_date(col(\"invoice_date\"), \"dd/MM/yyyy\"), \"yyyy\"))\n",
    "vendas_por_ano = df_vendas.groupBy(\"year\")\\\n",
    "                    .agg(sum(\"sales_amount\").alias(\"valor_total_vendas\"))\\\n",
    "                    .orderBy('year')\\\n",
    "                    .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\n",
    "\n",
    "vendas_por_ano.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------+------------------+\n",
      "|          customer|country|valor_total_vendas|\n",
      "+------------------+-------+------------------+\n",
      "| Paracel Gigaplace|     US|       11397206.36|\n",
      "|           Pereras|     US|       10843991.23|\n",
      "|         Talarians|   null|        9254771.72|\n",
      "|PageWave Megastore|   null|        8707904.14|\n",
      "|   Target Gigstore|     AU|        5433005.93|\n",
      "|Userland Maxistore|   null|        5202201.60|\n",
      "|  Tandy Superstore|     US|        3275015.91|\n",
      "|          Vanstars|     US|        3251414.29|\n",
      "|   Acer Superstore|   null|        3122752.50|\n",
      "|    Kerite Company|   null|        3113493.93|\n",
      "+------------------+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 52956)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 269, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark/python/pyspark/serializers.py\", line 717, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# # 9 - tabela com customer, sales_amount, country. Ordenar melhores clientes pelo valor de sales_amount 10 melhores\n",
    "# # e mostrar o total de sales_amount dos 10 melhores\n",
    "# from pyspark.sql.functions import sum, col\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "df_joined = df_clientes.join(df_vendas, \"customer_key\")\n",
    "df_joined = df_joined.join(df_endereco, df_joined.customer_key == df_endereco.address_number,\"left\")\n",
    "df_resultado = df_joined.groupBy(\"customer\", 'country').agg(sum(\"sales_amount\").alias(\"valor_total_vendas\"))\n",
    "df_top10 = df_resultado.orderBy(\"valor_total_vendas\", ascending=False).withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2))).limit(10)\n",
    "df_top10.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
