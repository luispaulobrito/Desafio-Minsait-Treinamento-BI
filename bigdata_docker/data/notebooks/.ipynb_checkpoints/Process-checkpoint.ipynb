{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, dataframe\n",
    "from pyspark.sql.functions import when, col, sum, count, isnan, round\n",
    "from pyspark.sql.functions import regexp_replace, concat_ws, sha2, rtrim, substring\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_date\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import year, month, dayofmonth, quarter\n",
    "from pyspark.sql.types import DecimalType\n",
    "from pyspark.sql.functions import trim, regexp_replace, when, col\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_df(df, file):\n",
    "    output = \"desafio_curso/gold/\" + file\n",
    "    erase = \"hdfs dfs -rm \" + output + \"/*\"\n",
    "    rename = \"hdfs dfs -get /datalake/gold/\"+file+\"/part-* desafio_curso/gold/\"+file+\".csv\"\n",
    "    \n",
    "    print(rename)    \n",
    "    \n",
    "    df.coalesce(1).write\\\n",
    "        .format(\"csv\")\\\n",
    "        .option(\"header\", True)\\\n",
    "        .option(\"delimiter\", \";\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save(\"/datalake/gold/\"+file+\"/\")\n",
    "    \n",
    "    os.system(erase)\n",
    "    os.system(rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Carregar tabelas endereco e remover a primeira linha do cabeçalho\n",
    "\n",
    "df_endereco = spark.read.table(\"desafio_curso.endereco\")\n",
    "rdd = df_endereco.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_endereco = rdd.toDF(df_endereco.schema)\n",
    "\n",
    "df_clientes = spark.read.table(\"desafio_curso.clientes\")\n",
    "rdd = df_clientes.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_clientes = rdd.toDF(df_clientes.schema)\n",
    "\n",
    "df_divisao = spark.read.table(\"desafio_curso.divisao\")\n",
    "rdd = df_divisao.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_divisao = rdd.toDF(df_divisao.schema)\n",
    "\n",
    "df_regiao = spark.read.table(\"desafio_curso.regiao\")\n",
    "rdd = df_regiao.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_regiao = rdd.toDF(df_regiao.schema)\n",
    "\n",
    "df_vendas = spark.read.table(\"desafio_curso.vendas\")\n",
    "rdd = df_vendas.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_vendas = rdd.toDF(df_vendas.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpar linhas duplicadas em clientes e endereço\n",
    "df_clientes = df_clientes.dropDuplicates(['customer_key'])\n",
    "df_endereco = df_endereco.dropDuplicates(['address_number'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Junção das tabelas\n",
    "\n",
    "df_vendas = df_vendas.withColumnRenamed(\"customer_key\", \"customer_key_vendas\")\n",
    "df_endereco = df_endereco.withColumnRenamed(\"address_number\", \"address_number_endereco\")\n",
    "df_divisao = df_divisao.withColumnRenamed(\"division\", \"division_divisao\")\n",
    "df_regiao = df_regiao.withColumnRenamed(\"region_code\", \"region_code_regiao\")\n",
    "\n",
    "df_stage = df_vendas.join(df_clientes,df_vendas.customer_key_vendas == df_clientes.customer_key,\"left\")\n",
    "df_stage = df_stage.join(df_endereco,df_stage.address_number == df_endereco.address_number_endereco,\"left\")\n",
    "df_stage = df_stage.join(df_divisao,df_stage.division == df_divisao.division_divisao,\"left\")\n",
    "df_stage = df_stage.join(df_regiao,df_stage.region_code == df_regiao.region_code_regiao,\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicionar colunas Ano, Mês, Dia, Trimestre tomando como base a coluna invoice_date\n",
    "\n",
    "df_stage = df_stage.withColumn('invoice_date', to_date(col('invoice_date'), 'dd/MM/yyyy'))\n",
    "\n",
    "df_stage = df_stage \\\n",
    "    .withColumn('Ano', year('invoice_date')) \\\n",
    "    .withColumn('Mes', month('invoice_date')) \\\n",
    "    .withColumn('Dia', dayofmonth('invoice_date')) \\\n",
    "    .withColumn('Trimestre', quarter('invoice_date'))                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Campos decimais ou inteiros nulos ou vazios, sendo preenchidos por 0.\n",
    "\n",
    "cols_to_check = ['item_number', 'discount_amount', 'list_price', 'sales_amount', 'sales_amount_based_on_list_price', 'sales_cost_amount', 'sales_margin_amount', 'sales_price', 'line_number', 'sales_quantity']\n",
    "\n",
    "for col_name in cols_to_check:\n",
    "     df_stage = df_stage.withColumn(col_name, when(col(col_name) == '', 0).otherwise(col(col_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Campos strings vazios preenchidos com 'Não informado'\n",
    "\n",
    "all_columns = df_stage.columns\n",
    "\n",
    "for column in all_columns:\n",
    "   df_stage = df_stage.withColumn(column, \n",
    "                                  when(trim(regexp_replace(col(column), '\\n', 'null')) == \"\", \"Nao Informado\")\n",
    "                                  .otherwise(col(column)))\n",
    "\n",
    "#Campos strings nulos preenchidos com 'Não informado'\n",
    "df_stage = df_stage.fillna(\"Nao Informado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicionar chaves estrangeiras\n",
    "df_stage = df_stage.withColumn('PK_TEMPO', sha2(concat_ws(\"\",df_stage.invoice_date, df_stage.Ano,df_stage.Mes,df_stage.Dia,df_stage.Trimestre), 256))\n",
    "\n",
    "df_stage = df_stage.withColumn('PK_CLIENTES', sha2(concat_ws(\"\",df_stage.customer_key,df_stage.customer,df_stage.business_family_name,df_stage.business_unit,df_stage.customer_type,df_stage.division,df_stage.line_of_business,df_stage.phone,df_stage.region_code,df_stage.regional_sales_mgr,df_stage.search_type), 256))\n",
    "\n",
    "df_stage = df_stage.withColumn('PK_LOCALIDADE', sha2(concat_ws(\"\",df_stage.address_number,df_stage.city,df_stage.country,df_stage.state,df_stage.zip_code,df_stage.division,df_stage.division_name,df_stage.region_code,df_stage.region_name,df_stage.customer_address_1,df_stage.customer_address_2,df_stage.customer_address_3,df_stage.customer_address_4), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage.createOrReplaceTempView(\"stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerar dimensões e fato\n",
    "df_tempo = spark.sql(\"SELECT DISTINCT PK_TEMPO, invoice_date, Ano, Mes, Dia, Trimestre FROM stage\") \n",
    "df_clientes = spark.sql(\"SELECT DISTINCT PK_CLIENTES, customer_key, customer, business_family_name, business_unit, customer_type, division, line_of_business, phone, region_code, regional_sales_mgr, search_type FROM stage\")\n",
    "df_localidade = spark.sql(\"SELECT DISTINCT PK_LOCALIDADE, address_number, city, country, state, zip_code, division, division_name, region_code, region_name, customer_address_1, customer_address_2, customer_address_3, customer_address_4 FROM stage\")\n",
    "ft_vendas = spark.sql(\"SELECT PK_CLIENTES, PK_TEMPO, PK_LOCALIDADE, actual_delivery_date, date_key, discount_amount, invoice_date, invoice_number, item_class, item_number, item, line_number, list_price, order_number, promised_delivery_date, sales_amount AS valor_de_venda, sales_amount_based_on_list_price, sales_cost_amount, sales_margin_amount, sales_price, sales_quantity AS quantidade, sales_rep, u_m FROM stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs dfs -get /datalake/gold/dim_tempo/part-* desafio_curso/gold/dim_tempo.csv\n",
      "hdfs dfs -get /datalake/gold/dim_clientes/part-* desafio_curso/gold/dim_clientes.csv\n",
      "hdfs dfs -get /datalake/gold/dim_localidade/part-* desafio_curso/gold/dim_localidade.csv\n",
      "hdfs dfs -get /datalake/gold/ft_vendas/part-* desafio_curso/gold/ft_vendas.csv\n"
     ]
    }
   ],
   "source": [
    "#Exportar dataframes como tabelas csv\n",
    "salvar_df(df_tempo, 'dim_tempo')\n",
    "salvar_df(df_clientes, 'dim_clientes')\n",
    "salvar_df(df_localidade, 'dim_localidade')\n",
    "salvar_df(ft_vendas, 'ft_vendas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de valor de vendas:\n",
      "+--------------------+\n",
      "|total_vendas_decimal|\n",
      "+--------------------+\n",
      "|        186186769.05|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TESTES\n",
    "\n",
    "# 1 - soma de sales_amount\n",
    "print(\"Total de valor de vendas:\")\n",
    "ft_vendas = ft_vendas.withColumn(\"valor_de_venda\", regexp_replace(\"valor_de_venda\", \",\", \".\"))\n",
    "resultado = ft_vendas.agg({\"valor_de_venda\": \"sum\"}).withColumnRenamed(\"sum(valor_de_venda)\", \"total_vendas\")\n",
    "resultado_decimal = resultado.select(resultado[\"total_vendas\"].cast(DecimalType(18, 2)).alias(\"total_vendas_decimal\"))\n",
    "resultado_decimal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total produtos vendidos:\n",
      "+------------+\n",
      "|total_vendas|\n",
      "+------------+\n",
      "|   2943194.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 - soma de sales_quantity\n",
    "print(\"Total produtos vendidos:\")\n",
    "total_vendas = ft_vendas.agg(sum('quantidade').alias('total_vendas'))\n",
    "total_vendas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produto mais vendido:\n",
      "+--------------------+----------------+\n",
      "|                item|quantidade_total|\n",
      "+--------------------+----------------+\n",
      "|Better Large Cann...|        590343.0|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - produto mais vendido\n",
    "print(\"Produto mais vendido:\")\n",
    "agrupado_por_item = ft_vendas.groupBy('item').agg(sum('quantidade').alias('quantidade_total'))\n",
    "produto_mais_vendido = agrupado_por_item.orderBy('quantidade_total', ascending=False).limit(1)\n",
    "produto_mais_vendido.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Produtos mais vendidos:\n",
      "+--------------------+----------------+\n",
      "|                item|quantidade_total|\n",
      "+--------------------+----------------+\n",
      "|Better Large Cann...|        590343.0|\n",
      "|High Top Dried Mu...|        377259.0|\n",
      "|Better Canned Tun...|        266996.0|\n",
      "|   Walrus Chardonnay|        212022.0|\n",
      "|Red Spade Pimento...|        163296.0|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 - 5 produtos mais sales_quantity\n",
    "print(\"5 Produtos mais vendidos:\")\n",
    "resultado = ft_vendas.groupBy('item').agg(sum('quantidade').alias('quantidade_total')) \\\n",
    "            .orderBy('quantidade_total', ascending=False) \\\n",
    "            .limit(5)\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Produtos com maior valor de venda:\n",
      "+--------------------+------------------+\n",
      "|                item|valor_total_vendas|\n",
      "+--------------------+------------------+\n",
      "|Better Large Cann...|       15454172.47|\n",
      "|High Top Dried Mu...|       13368414.53|\n",
      "|Red Spade Pimento...|        5711486.45|\n",
      "|Better Canned Tun...|        5693075.12|\n",
      "|        Ebony Squash|        5380727.75|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 - 5 produtos mais sales_amount\n",
    "print(\"5 Produtos com maior valor de venda:\")\n",
    "resultado = ft_vendas.groupBy('item').agg(sum('valor_de_venda').alias('valor_total_vendas')) \\\n",
    "            .orderBy('valor_total_vendas', ascending=False) \\\n",
    "            .limit(5) \\\n",
    "            .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\n",
    "\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de venda e quantidade de vendas por mês:\n",
      "+-------------+----------------+------------------+\n",
      "|          mes|quantidade_total|valor_total_vendas|\n",
      "+-------------+----------------+------------------+\n",
      "|            1|        325560.0|       19471739.54|\n",
      "|           10|        184381.0|       12829983.51|\n",
      "|           11|        221260.0|       13794762.06|\n",
      "|           12|        213505.0|       14516466.67|\n",
      "|            2|        327750.0|       20497349.91|\n",
      "|            3|        328798.0|       21714172.68|\n",
      "|            4|        193063.0|       12112134.49|\n",
      "|            5|        202787.0|       11053298.15|\n",
      "|            6|        264470.0|       15852396.38|\n",
      "|            7|        196932.0|       13287585.39|\n",
      "|            8|        242825.0|       14590611.40|\n",
      "|            9|        241863.0|       16466268.87|\n",
      "|Nao Informado|             0.0|              0.00|\n",
      "+-------------+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6 - sales_quantity por mes e  sales_amount por mes\n",
    "print(\"Valor de venda e quantidade de vendas por mês:\")\n",
    "resultado = ft_vendas.join(df_tempo, ft_vendas.PK_TEMPO == df_tempo.PK_TEMPO) \\\n",
    "            .groupBy('mes') \\\n",
    "            .agg(sum('quantidade').alias('quantidade_total'), sum('valor_de_venda').alias('valor_total_vendas')) \\\n",
    "            .orderBy('mes')\\\n",
    "            .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+-----------+\n",
      "|      country|valor_total_vendas|porcentagem|\n",
      "+-------------+------------------+-----------+\n",
      "|           US|       99813239.18|      53.61|\n",
      "|Nao Informado|       51426569.95|      27.62|\n",
      "|           UK|       15555235.25|       8.35|\n",
      "|           AU|       10962238.21|       5.89|\n",
      "|           CA|        6320257.36|       3.39|\n",
      "|           IR|        2109229.10|       1.13|\n",
      "+-------------+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 7 - sales quantity por country - mostrar porcentagem\n",
    "from pyspark.sql.functions import sum, format_string\n",
    "total_vendas = ft_vendas.agg({\"valor_de_venda\": \"sum\"}).collect()[0][0]\n",
    "\n",
    "resultado = ft_vendas.join(df_localidade, ft_vendas.PK_LOCALIDADE == df_localidade.PK_LOCALIDADE) \\\n",
    "            .groupBy('country') \\\n",
    "            .agg(sum('valor_de_venda').alias('valor_total_vendas'), (sum('valor_de_venda')/total_vendas*100).alias('porcentagem')) \\\n",
    "            .orderBy('porcentagem', ascending=False)\\\n",
    "            .withColumn('porcentagem', col('porcentagem').cast(DecimalType(18, 2)))\\\n",
    "            .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de venda por ano:\n",
      "+-------------+------------------+\n",
      "|          ano|valor_total_vendas|\n",
      "+-------------+------------------+\n",
      "|         2017|       77906591.65|\n",
      "|         2018|       87462706.40|\n",
      "|         2019|       20817471.00|\n",
      "|Nao Informado|              0.00|\n",
      "+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8 - sales_amount por ano\n",
    "print(\"Valor de venda por ano:\")\n",
    "resultado = ft_vendas.join(df_tempo, ft_vendas.PK_TEMPO == df_tempo.PK_TEMPO) \\\n",
    "            .groupBy('ano') \\\n",
    "            .agg(sum('valor_de_venda').alias('valor_total_vendas')) \\\n",
    "            .orderBy('ano')\\\n",
    "            .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 - tabela com customer, sales_amount, country. Ordenar melhores clientes pelo valor de sales_amount 10 melhores\n",
    "# e mostrar o total de sales_amount dos 10 melhores\n",
    "resultado = ft_vendas.join(df_clientes, ft_vendas.PK_CLIENTES == df_clientes.PK_CLIENTES) \\\n",
    "            .join(df_localidade, ft_vendas.PK_LOCALIDADE == df_localidade.PK_LOCALIDADE) \\\n",
    "            .groupBy('customer', 'country') \\\n",
    "            .agg(sum('valor_de_venda').alias('valor_total_vendas')) \\\n",
    "            .orderBy('valor_total_vendas', ascending=False) \\\n",
    "            .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\\\n",
    "            .limit(10)\n",
    "resultado.show()\n",
    "total_vendas = resultado.agg({\"valor_total_vendas\": \"sum\"}).collect()[0][0]\n",
    "print(\"Total de vendas dos 10 melhores clientes: \", total_vendas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
