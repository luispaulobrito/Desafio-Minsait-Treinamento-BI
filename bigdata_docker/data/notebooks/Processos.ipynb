{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, dataframe\n",
    "from pyspark.sql.functions import when, col, sum, count, isnan, round\n",
    "from pyspark.sql.functions import regexp_replace, concat_ws, sha2, rtrim, substring\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_date\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import year, month, dayofmonth, quarter\n",
    "from pyspark.sql.types import DecimalType\n",
    "from pyspark.sql.functions import trim, regexp_replace, when, col\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_df(df, file):\n",
    "    output = \"input/projeto-hive/gold/\" + file\n",
    "    erase = \"hdfs dfs -rm \" + output + \"/*\"\n",
    "    rename = \"hdfs dfs -get /datalake/gold/\"+file+\"/part-* input/projeto-hive/gold/\"+file+\".csv\"\n",
    "    \n",
    "    print(rename)    \n",
    "    \n",
    "    df.coalesce(1).write\\\n",
    "        .format(\"csv\")\\\n",
    "        .option(\"header\", True)\\\n",
    "        .option(\"delimiter\", \";\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save(\"/datalake/gold/\"+file+\"/\")\n",
    "    \n",
    "    os.system(erase)\n",
    "    os.system(rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Carregar tabelas endereco e remover a primeira linha do cabeçalho\n",
    "\n",
    "df_endereco = spark.read.table(\"desafio_curso.endereco\")\n",
    "rdd = df_endereco.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_endereco = rdd.toDF(df_endereco.schema)\n",
    "\n",
    "df_clientes = spark.read.table(\"desafio_curso.clientes\")\n",
    "rdd = df_clientes.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_clientes = rdd.toDF(df_clientes.schema)\n",
    "\n",
    "df_divisao = spark.read.table(\"desafio_curso.divisao\")\n",
    "rdd = df_divisao.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_divisao = rdd.toDF(df_divisao.schema)\n",
    "\n",
    "df_regiao = spark.read.table(\"desafio_curso.regiao\")\n",
    "rdd = df_regiao.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_regiao = rdd.toDF(df_regiao.schema)\n",
    "\n",
    "df_vendas = spark.read.table(\"desafio_curso.vendas\")\n",
    "rdd = df_vendas.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_vendas = rdd.toDF(df_vendas.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpar linhas duplicadas em clientes e endereço\n",
    "df_clientes = df_clientes.dropDuplicates(['customer_key'])\n",
    "df_endereco = df_endereco.dropDuplicates(['address_number'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Junção das tabelas\n",
    "\n",
    "df_vendas = df_vendas.withColumnRenamed(\"customer_key\", \"customer_key_vendas\")\n",
    "df_endereco = df_endereco.withColumnRenamed(\"address_number\", \"address_number_endereco\")\n",
    "df_divisao = df_divisao.withColumnRenamed(\"division\", \"division_divisao\")\n",
    "df_regiao = df_regiao.withColumnRenamed(\"region_code\", \"region_code_regiao\")\n",
    "\n",
    "df_stage = df_vendas.join(df_clientes,df_vendas.customer_key_vendas == df_clientes.customer_key,\"left\")\n",
    "df_stage = df_stage.join(df_endereco,df_stage.address_number == df_endereco.address_number_endereco,\"left\")\n",
    "df_stage = df_stage.join(df_divisao,df_stage.division == df_divisao.division_divisao,\"left\")\n",
    "df_stage = df_stage.join(df_regiao,df_stage.region_code == df_regiao.region_code_regiao,\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicionar colunas Ano, Mês, Dia, Trimestre tomando como base a coluna invoice_date\n",
    "\n",
    "df_stage = df_stage.withColumn('invoice_date', to_date(col('invoice_date'), 'dd/MM/yyyy'))\n",
    "\n",
    "df_stage = df_stage \\\n",
    "    .withColumn('Ano', year('invoice_date')) \\\n",
    "    .withColumn('Mes', month('invoice_date')) \\\n",
    "    .withColumn('Dia', dayofmonth('invoice_date')) \\\n",
    "    .withColumn('Trimestre', quarter('invoice_date'))                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Campos decimais ou inteiros nulos ou vazios, sendo preenchidos por 0.\n",
    "\n",
    "cols_to_check = ['item_number', 'discount_amount', 'list_price', 'sales_amount', 'sales_amount_based_on_list_price', 'sales_cost_amount', 'sales_margin_amount', 'sales_price', 'line_number', 'sales_quantity']\n",
    "\n",
    "for col_name in cols_to_check:\n",
    "     df_stage = df_stage.withColumn(col_name, when(col(col_name) == '', 0).otherwise(col(col_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Campos strings vazios preenchidos com 'Não informado'\n",
    "\n",
    "all_columns = df_stage.columns\n",
    "\n",
    "for column in all_columns:\n",
    "   df_stage = df_stage.withColumn(column, \n",
    "                                  when(trim(regexp_replace(col(column), '\\n', 'null')) == \"\", \"Nao Informado\")\n",
    "                                  .otherwise(col(column)))\n",
    "\n",
    "#Campos strings nulos preenchidos com 'Não informado'\n",
    "df_stage = df_stage.fillna(\"Nao Informado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicionar chaves estrangeiras\n",
    "df_stage = df_stage.withColumn('PK_TEMPO', sha2(concat_ws(\"\",df_stage.invoice_date, df_stage.Ano,df_stage.Mes,df_stage.Dia,df_stage.Trimestre), 256))\n",
    "\n",
    "df_stage = df_stage.withColumn('PK_CLIENTES', sha2(concat_ws(\"\",df_stage.customer_key,df_stage.customer,df_stage.business_family_name,df_stage.business_unit,df_stage.customer_type,df_stage.division,df_stage.line_of_business,df_stage.phone,df_stage.region_code,df_stage.regional_sales_mgr,df_stage.search_type), 256))\n",
    "\n",
    "df_stage = df_stage.withColumn('PK_LOCALIDADE', sha2(concat_ws(\"\",df_stage.address_number,df_stage.city,df_stage.country,df_stage.state,df_stage.zip_code,df_stage.division,df_stage.division_name,df_stage.region_code,df_stage.region_name,df_stage.customer_address_1,df_stage.customer_address_2,df_stage.customer_address_3,df_stage.customer_address_4), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage.createOrReplaceTempView(\"stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerar dimensões e fato\n",
    "df_tempo = spark.sql(\"SELECT DISTINCT PK_TEMPO, invoice_date, Ano, Mes, Dia, Trimestre FROM stage\") \n",
    "df_clientes = spark.sql(\"SELECT DISTINCT PK_CLIENTES, customer_key, customer, business_family_name, business_unit, customer_type, division, line_of_business, phone, region_code, regional_sales_mgr, search_type FROM stage\")\n",
    "df_localidade = spark.sql(\"SELECT DISTINCT PK_LOCALIDADE, address_number, city, country, state, zip_code, division, division_name, region_code, region_name, customer_address_1, customer_address_2, customer_address_3, customer_address_4 FROM stage\")\n",
    "ft_vendas = spark.sql(\"SELECT PK_CLIENTES, PK_TEMPO, PK_LOCALIDADE, actual_delivery_date, date_key, discount_amount, invoice_date, invoice_number, item_class, item_number, item, line_number, list_price, order_number, promised_delivery_date, sales_amount AS valor_de_venda, sales_amount_based_on_list_price, sales_cost_amount, sales_margin_amount, sales_price, sales_quantity AS quantidade, sales_rep, u_m FROM stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs dfs -get /datalake/gold/dim_tempo/part-* input/projeto-hive/gold/dim_tempo.csv\n"
     ]
    }
   ],
   "source": [
    "#Exportar dataframes como tabelas csv\n",
    "salvar_df(df_tempo, 'dim_tempo')\n",
    "salvar_df(df_clientes, 'dim_clientes')\n",
    "salvar_df(df_localidade, 'dim_localidade')\n",
    "salvar_df(ft_vendas, 'ft_vendas')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
