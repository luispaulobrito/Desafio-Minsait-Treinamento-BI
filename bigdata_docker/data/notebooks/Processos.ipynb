{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, dataframe\n",
    "from pyspark.sql.functions import when, col, sum, count, isnan, round\n",
    "from pyspark.sql.functions import regexp_replace, concat_ws, sha2, rtrim, substring\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_date\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import year, month, dayofmonth, quarter\n",
    "from pyspark.sql.types import DecimalType\n",
    "from pyspark.sql.functions import trim, regexp_replace, when, col\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_df(df, file):\n",
    "    output = \"input/projeto-hive/gold/\" + file\n",
    "    erase = \"hdfs dfs -rm \" + output + \"/*\"\n",
    "    rename = \"hdfs dfs -get /datalake/gold/\"+file+\"/part-* input/projeto-hive/gold/\"+file+\".csv\"\n",
    "    \n",
    "    print(rename)    \n",
    "    \n",
    "    df.coalesce(1).write\\\n",
    "        .format(\"csv\")\\\n",
    "        .option(\"header\", True)\\\n",
    "        .option(\"delimiter\", \";\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save(\"/datalake/gold/\"+file+\"/\")\n",
    "    \n",
    "    os.system(erase)\n",
    "    os.system(rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Carregar tabelas endereco e remover a primeira linha do cabeçalho\n",
    "\n",
    "df_endereco = spark.read.table(\"desafio_curso.endereco\")\n",
    "rdd = df_endereco.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_endereco = rdd.toDF(df_endereco.schema)\n",
    "\n",
    "df_clientes = spark.read.table(\"desafio_curso.clientes\")\n",
    "rdd = df_clientes.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_clientes = rdd.toDF(df_clientes.schema)\n",
    "\n",
    "df_divisao = spark.read.table(\"desafio_curso.divisao\")\n",
    "rdd = df_divisao.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_divisao = rdd.toDF(df_divisao.schema)\n",
    "\n",
    "df_regiao = spark.read.table(\"desafio_curso.regiao\")\n",
    "rdd = df_regiao.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_regiao = rdd.toDF(df_regiao.schema)\n",
    "\n",
    "df_vendas = spark.read.table(\"desafio_curso.vendas\")\n",
    "rdd = df_vendas.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_vendas = rdd.toDF(df_vendas.schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpar linhas duplicadas em clientes e endereço\n",
    "df_clientes = df_clientes.dropDuplicates(['customer_key'])\n",
    "df_endereco = df_endereco.dropDuplicates(['address_number'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Junção das tabelas\n",
    "\n",
    "df_vendas = df_vendas.withColumnRenamed(\"customer_key\", \"customer_key_vendas\")\n",
    "df_endereco = df_endereco.withColumnRenamed(\"address_number\", \"address_number_endereco\")\n",
    "df_divisao = df_divisao.withColumnRenamed(\"division\", \"division_divisao\")\n",
    "df_regiao = df_regiao.withColumnRenamed(\"region_code\", \"region_code_regiao\")\n",
    "\n",
    "df_stage = df_vendas.join(df_clientes,df_vendas.customer_key_vendas == df_clientes.customer_key,\"left\")\n",
    "df_stage = df_stage.join(df_endereco,df_stage.address_number == df_endereco.address_number_endereco,\"left\")\n",
    "df_stage = df_stage.join(df_divisao,df_stage.division == df_divisao.division_divisao,\"left\")\n",
    "df_stage = df_stage.join(df_regiao,df_stage.region_code == df_regiao.region_code_regiao,\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicionar colunas Ano, Mês, Dia, Trimestre tomando como base a coluna invoice_date\n",
    "\n",
    "df_stage = df_stage.withColumn('invoice_date', to_date(col('invoice_date'), 'dd/MM/yyyy'))\n",
    "\n",
    "df_stage = df_stage \\\n",
    "    .withColumn('Ano', year('invoice_date')) \\\n",
    "    .withColumn('Mes', month('invoice_date')) \\\n",
    "    .withColumn('Dia', dayofmonth('invoice_date')) \\\n",
    "    .withColumn('Trimestre', quarter('invoice_date'))                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Campos decimais ou inteiros nulos ou vazios, sendo preenchidos por 0.\n",
    "\n",
    "cols_to_check = ['item_number', 'discount_amount', 'list_price', 'sales_amount', 'sales_amount_based_on_list_price', 'sales_cost_amount', 'sales_margin_amount', 'sales_price', 'line_number', 'sales_quantity']\n",
    "\n",
    "for col_name in cols_to_check:\n",
    "     df_stage = df_stage.withColumn(col_name, when(col(col_name) == '', 0).otherwise(col(col_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Campos strings vazios preenchidos com 'Não informado'\n",
    "\n",
    "all_columns = df_stage.columns\n",
    "\n",
    "for column in all_columns:\n",
    "   df_stage = df_stage.withColumn(column, \n",
    "                                  when(trim(regexp_replace(col(column), '\\n', 'null')) == \"\", \"Nao Informado\")\n",
    "                                  .otherwise(col(column)))\n",
    "\n",
    "#Campos strings nulos preenchidos com 'Não informado'\n",
    "df_stage = df_stage.fillna(\"Nao Informado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicionar chaves estrangeiras\n",
    "df_stage = df_stage.withColumn('PK_TEMPO', sha2(concat_ws(\"\",df_stage.invoice_date, df_stage.Ano,df_stage.Mes,df_stage.Dia,df_stage.Trimestre), 256))\n",
    "\n",
    "df_stage = df_stage.withColumn('PK_CLIENTES', sha2(concat_ws(\"\",df_stage.customer_key,df_stage.customer,df_stage.business_family_name,df_stage.business_unit,df_stage.customer_type,df_stage.division,df_stage.line_of_business,df_stage.phone,df_stage.region_code,df_stage.regional_sales_mgr,df_stage.search_type), 256))\n",
    "\n",
    "df_stage = df_stage.withColumn('PK_LOCALIDADE', sha2(concat_ws(\"\",df_stage.address_number,df_stage.city,df_stage.country,df_stage.state,df_stage.zip_code,df_stage.division,df_stage.division_name,df_stage.region_code,df_stage.region_name,df_stage.customer_address_1,df_stage.customer_address_2,df_stage.customer_address_3,df_stage.customer_address_4), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage.createOrReplaceTempView(\"stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerar dimensões e fato\n",
    "df_tempo = spark.sql(\"SELECT DISTINCT PK_TEMPO, invoice_date, Ano, Mes, Dia, Trimestre FROM stage\") \n",
    "df_clientes = spark.sql(\"SELECT DISTINCT PK_CLIENTES, customer_key, customer, business_family_name, business_unit, customer_type, division, line_of_business, phone, region_code, regional_sales_mgr, search_type FROM stage\")\n",
    "df_localidade = spark.sql(\"SELECT DISTINCT PK_LOCALIDADE, address_number, city, country, state, zip_code, division, division_name, region_code, region_name, customer_address_1, customer_address_2, customer_address_3, customer_address_4 FROM stage\")\n",
    "ft_vendas = spark.sql(\"SELECT PK_CLIENTES, PK_TEMPO, PK_LOCALIDADE, actual_delivery_date, date_key, discount_amount, invoice_date, invoice_number, item_class, item_number, item, line_number, list_price, order_number, promised_delivery_date, sales_amount AS valor_de_venda, sales_amount_based_on_list_price, sales_cost_amount, sales_margin_amount, sales_price, sales_quantity AS quantidade, sales_rep, u_m FROM stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs dfs -get /datalake/gold/dim_tempo/part-* input/projeto-hive/gold/dim_tempo.csv\n",
      "hdfs dfs -get /datalake/gold/dim_clientes/part-* input/projeto-hive/gold/dim_clientes.csv\n",
      "hdfs dfs -get /datalake/gold/dim_localidade/part-* input/projeto-hive/gold/dim_localidade.csv\n",
      "hdfs dfs -get /datalake/gold/ft_vendas/part-* input/projeto-hive/gold/ft_vendas.csv\n"
     ]
    }
   ],
   "source": [
    "#Exportar dataframes como tabelas csv\n",
    "salvar_df(df_tempo, 'dim_tempo')\n",
    "salvar_df(df_clientes, 'dim_clientes')\n",
    "salvar_df(df_localidade, 'dim_localidade')\n",
    "salvar_df(ft_vendas, 'ft_vendas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de valor de vendas:\n",
      "+--------------------+\n",
      "|total_vendas_decimal|\n",
      "+--------------------+\n",
      "|        186186769.05|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TESTES\n",
    "\n",
    "# 1 - soma de sales_amount\n",
    "print(\"Total de valor de vendas:\")\n",
    "ft_vendas = ft_vendas.withColumn(\"valor_de_venda\", regexp_replace(\"valor_de_venda\", \",\", \".\"))\n",
    "resultado = ft_vendas.agg({\"valor_de_venda\": \"sum\"}).withColumnRenamed(\"sum(valor_de_venda)\", \"total_vendas\")\n",
    "resultado_decimal = resultado.select(resultado[\"total_vendas\"].cast(DecimalType(18, 2)).alias(\"total_vendas_decimal\"))\n",
    "resultado_decimal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total produtos vendidos:\n",
      "+------------+\n",
      "|total_vendas|\n",
      "+------------+\n",
      "|   2943194.0|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2 - soma de sales_quantity\n",
    "print(\"Total produtos vendidos:\")\n",
    "total_vendas = ft_vendas.agg(sum('quantidade').alias('total_vendas'))\n",
    "total_vendas.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produto mais vendido:\n",
      "+--------------------+----------------+\n",
      "|                item|quantidade_total|\n",
      "+--------------------+----------------+\n",
      "|Better Large Cann...|        590343.0|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 - produto mais vendido\n",
    "print(\"Produto mais vendido:\")\n",
    "agrupado_por_item = ft_vendas.groupBy('item').agg(sum('quantidade').alias('quantidade_total'))\n",
    "produto_mais_vendido = agrupado_por_item.orderBy('quantidade_total', ascending=False).limit(1)\n",
    "produto_mais_vendido.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Produtos mais vendidos:\n",
      "+--------------------+----------------+\n",
      "|                item|quantidade_total|\n",
      "+--------------------+----------------+\n",
      "|Better Large Cann...|        590343.0|\n",
      "|High Top Dried Mu...|        377259.0|\n",
      "|Better Canned Tun...|        266996.0|\n",
      "|   Walrus Chardonnay|        212022.0|\n",
      "|Red Spade Pimento...|        163296.0|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4 - 5 produtos mais sales_quantity\n",
    "print(\"5 Produtos mais vendidos:\")\n",
    "resultado = ft_vendas.groupBy('item').agg(sum('quantidade').alias('quantidade_total')) \\\n",
    "            .orderBy('quantidade_total', ascending=False) \\\n",
    "            .limit(5)\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Produtos com maior valor de venda:\n",
      "+--------------------+------------------+\n",
      "|                item|valor_total_vendas|\n",
      "+--------------------+------------------+\n",
      "|Better Large Cann...|       15454172.47|\n",
      "|High Top Dried Mu...|       13368414.53|\n",
      "|Red Spade Pimento...|        5711486.45|\n",
      "|Better Canned Tun...|        5693075.12|\n",
      "|        Ebony Squash|        5380727.75|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 5 - 5 produtos mais sales_amount\n",
    "print(\"5 Produtos com maior valor de venda:\")\n",
    "resultado = ft_vendas.groupBy('item').agg(sum('valor_de_venda').alias('valor_total_vendas')) \\\n",
    "            .orderBy('valor_total_vendas', ascending=False) \\\n",
    "            .limit(5) \\\n",
    "            .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\n",
    "\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valor de venda e quantidade de vendas por mês:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 41370)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1159, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 985, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1164, in send_command\n",
      "    \"Error while receiving\", e, proto.ERROR_ON_RECEIVE)\n",
      "py4j.protocol.Py4JNetworkError: Error while receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.6/socketserver.py\", line 320, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.6/socketserver.py\", line 351, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/anaconda3/lib/python3.6/socketserver.py\", line 364, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/anaconda3/lib/python3.6/socketserver.py\", line 724, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 269, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 241, in poll\n",
      "    if func():\n",
      "  File \"/opt/spark/python/pyspark/accumulators.py\", line 245, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/opt/spark/python/pyspark/serializers.py\", line 717, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o968.showString",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-c66fb94c0189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Valor de venda e quantidade de vendas por mês:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresultado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mft_vendas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_tempo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_vendas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPK_TEMPO\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdf_tempo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPK_TEMPO\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mes'\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'quantidade'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'quantidade_total'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valor_de_venda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valor_total_vendas'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mes'\u001b[0m\u001b[0;34m)\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valor_total_vendas'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'valor_total_vendas'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDecimalType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mresultado\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    334\u001b[0m             raise Py4JError(\n\u001b[1;32m    335\u001b[0m                 \u001b[0;34m\"An error occurred while calling {0}{1}{2}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 format(target_id, \".\", name))\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o968.showString"
     ]
    }
   ],
   "source": [
    "# 6 - sales_quantity por mes e  sales_amount por mes\n",
    "print(\"Valor de venda e quantidade de vendas por mês:\")\n",
    "resultado = ft_vendas.join(df_tempo, ft_vendas.PK_TEMPO == df_tempo.PK_TEMPO) \\\n",
    "            .groupBy('mes') \\\n",
    "            .agg(sum('quantidade').alias('quantidade_total'), sum('valor_de_venda').alias('valor_total_vendas')) \\\n",
    "            .orderBy('mes')\\\n",
    "            .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7 - sales quantity por country - mostrar porcentagem\n",
    "from pyspark.sql.functions import sum, format_string\n",
    "total_vendas = ft_vendas.agg({\"valor_de_venda\": \"sum\"}).collect()[0][0]\n",
    "\n",
    "resultado = ft_vendas.join(df_localidade, ft_vendas.PK_LOCALIDADE == df_localidade.PK_LOCALIDADE) \\\n",
    "            .groupBy('country') \\\n",
    "            .agg(sum('valor_de_venda').alias('valor_total_vendas'), (sum('valor_de_venda')/total_vendas*100).alias('porcentagem')) \\\n",
    "            .orderBy('porcentagem', ascending=False)\\\n",
    "            .withColumn('porcentagem', col('porcentagem').cast(DecimalType(18, 2)))\\\n",
    "            .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 - sales_amount por ano\n",
    "print(\"Valor de venda por ano:\")\n",
    "resultado = ft_vendas.join(df_tempo, ft_vendas.PK_TEMPO == df_tempo.PK_TEMPO) \\\n",
    "            .groupBy('ano') \\\n",
    "            .agg(sum('valor_de_venda').alias('valor_total_vendas')) \\\n",
    "            .orderBy('ano')\\\n",
    "            .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\n",
    "resultado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9 - tabela com customer, sales_amount, country. Ordenar melhores clientes pelo valor de sales_amount 10 melhores\n",
    "# e mostrar o total de sales_amount dos 10 melhores\n",
    "resultado = ft_vendas.join(df_clientes, ft_vendas.PK_CLIENTES == df_clientes.PK_CLIENTES) \\\n",
    "            .join(df_localidade, ft_vendas.PK_LOCALIDADE == df_localidade.PK_LOCALIDADE) \\\n",
    "            .groupBy('customer', 'country') \\\n",
    "            .agg(sum('valor_de_venda').alias('valor_total_vendas')) \\\n",
    "            .orderBy('valor_total_vendas', ascending=False) \\\n",
    "            .withColumn('valor_total_vendas', col('valor_total_vendas').cast(DecimalType(18, 2)))\\\n",
    "            .limit(10)\n",
    "resultado.show()\n",
    "total_vendas = resultado.agg({\"valor_total_vendas\": \"sum\"}).collect()[0][0]\n",
    "print(\"Total de vendas dos 10 melhores clientes: \", total_vendas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
