{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, dataframe\n",
    "from pyspark.sql.functions import when, col, sum, count, isnan, round\n",
    "from pyspark.sql.functions import regexp_replace, concat_ws, sha2, rtrim, substring\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_date\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import year, month, dayofmonth, quarter\n",
    "from pyspark.sql.types import DecimalType\n",
    "from pyspark.sql.functions import trim, regexp_replace, when, col\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_df(df, file):\n",
    "    output = \"input/projeto-hive/gold/\" + file\n",
    "    erase = \"hdfs dfs -rm \" + output + \"/*\"\n",
    "    rename = \"hdfs dfs -get /datalake/gold/\"+file+\"/part-* input/projeto-hive/gold/\"+file+\".csv\"\n",
    "    \n",
    "    print(rename)    \n",
    "    \n",
    "    df.coalesce(1).write\\\n",
    "        .format(\"csv\")\\\n",
    "        .option(\"header\", True)\\\n",
    "        .option(\"delimiter\", \";\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save(\"/datalake/gold/\"+file+\"/\")\n",
    "    \n",
    "    os.system(erase)\n",
    "    os.system(rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Carregar tabela endereco e remover a primeira linha que copiou o cabeçalho\n",
    "df_endereco = spark.read.table(\"desafio_curso.endereco\")\n",
    "rdd = df_endereco.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_endereco = rdd.toDF(df_endereco.schema)\n",
    "\n",
    "# Carregar tabela clientes e remover a primeira linha que copiou o cabeçalho\n",
    "df_clientes = spark.read.table(\"desafio_curso.clientes\")\n",
    "rdd = df_clientes.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_clientes = rdd.toDF(df_clientes.schema)\n",
    "\n",
    "# Carregar tabela divisao e remover a primeira linha que copiou o cabeçalho\n",
    "df_divisao = spark.read.table(\"desafio_curso.divisao\")\n",
    "rdd = df_divisao.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_divisao = rdd.toDF(df_divisao.schema)\n",
    "\n",
    "# Carregar tabela regiao e remover a primeira linha que copiou o cabeçalho\n",
    "df_regiao = spark.read.table(\"desafio_curso.regiao\")\n",
    "rdd = df_regiao.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_regiao = rdd.toDF(df_regiao.schema)\n",
    "\n",
    "# Carregar tabela regiao e remover a primeira linha que copiou o cabeçalho\n",
    "df_vendas = spark.read.table(\"desafio_curso.vendas\")\n",
    "rdd = df_vendas.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "df_vendas = rdd.toDF(df_vendas.schema)\n",
    "\n",
    "# tables = [\"endereco\", \"clientes\", \"divisao\", \"regiao\", \"vendas\"]\n",
    "# dfs = {}\n",
    "\n",
    "# for table in tables:\n",
    "#     # Carregar a tabela e remover a primeira linha que copiou o cabeçalho\n",
    "#     df = spark.read.table(\"desafio_curso.\" + table)\n",
    "#     rdd = df.rdd.zipWithIndex().filter(lambda x: x[1] > 0).map(lambda x: x[0])\n",
    "#     df = rdd.toDF(df.schema)\n",
    "#     dfs[table] = df\n",
    "\n",
    "\n",
    "# df_vendas.limit(3).show()\n",
    "# df_endereco.limit(3).show()\n",
    "# df_clientes.limit(3).show()\n",
    "# df_divisao.show()\n",
    "# df_regiao.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import sum\n",
    "\n",
    "# # Somar os valores da coluna \"sales_amount\" no DataFrame \"df_stage\"\n",
    "# total_sales_amount = df_vendas.agg(sum(\"sales_amount\")).collect()[0][0]\n",
    "\n",
    "# # Imprimir o resultado\n",
    "# print(\"Total sales amount:\", total_sales_amount)\n",
    "\n",
    "# # Contar a quantidade de linhas no DataFrame \"df_vendas\"\n",
    "# # num_rows = df_vendas.count()\n",
    "# num_rows = df_vendas.count()\n",
    "\n",
    "# # Imprimir o resultado\n",
    "# print(\"Número de linhas:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpar linhas duplicadas em clientes e endereço\n",
    "df_clientes = df_clientes.dropDuplicates(['customer_key'])\n",
    "df_endereco = df_endereco.dropDuplicates(['address_number'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import sum\n",
    "\n",
    "# # Somar os valores da coluna \"sales_amount\" no DataFrame \"df_stage\"\n",
    "# total_sales_amount = df_vendas.agg(sum(\"sales_amount\")).collect()[0][0]\n",
    "\n",
    "# # Imprimir o resultado\n",
    "# print(\"Total sales amount:\", total_sales_amount)\n",
    "\n",
    "# # Contar a quantidade de linhas no DataFrame \"df_vendas\"\n",
    "# num_rows = df_vendas.count()\n",
    "\n",
    "# # Imprimir o resultado\n",
    "# print(\"Número de linhas:\", num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazer um tabelão\n",
    "\n",
    "df_vendas = df_vendas.withColumnRenamed(\"customer_key\", \"customer_key_vendas\")\n",
    "df_endereco = df_endereco.withColumnRenamed(\"address_number\", \"address_number_endereco\")\n",
    "df_divisao = df_divisao.withColumnRenamed(\"division\", \"division_divisao\")\n",
    "df_regiao = df_regiao.withColumnRenamed(\"region_code\", \"region_code_regiao\")\n",
    "\n",
    "df_stage = df_vendas.join(df_clientes,df_vendas.customer_key_vendas == df_clientes.customer_key,\"left\")\n",
    "df_stage = df_stage.join(df_endereco,df_stage.address_number == df_endereco.address_number_endereco,\"left\")\n",
    "df_stage = df_stage.join(df_divisao,df_stage.division == df_divisao.division_divisao,\"left\")\n",
    "df_stage = df_stage.join(df_regiao,df_stage.region_code == df_regiao.region_code_regiao,\"left\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import sum\n",
    "\n",
    "# # Somar os valores da coluna \"sales_amount\" no DataFrame \"df_stage\"\n",
    "# total_sales_amount = df_stage.agg(sum(\"sales_amount\")).collect()[0][0]\n",
    "# total_sales_quantity = df_stage.agg(sum(\"sales_quantity\")).collect()[0][0]\n",
    "\n",
    "# # Imprimir o resultado\n",
    "# print(\"Total sales amount:\", total_sales_amount)\n",
    "# print(\"Total sales quantity:\", total_sales_quantity)\n",
    "\n",
    "# print(df_vendas.count())\n",
    "# print(df_clientes.count())\n",
    "# print(df_stage.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicionar colunas Ano, Mês, Dia, Trimestre tomando como base a coluna invoice_date\n",
    "\n",
    "df_stage = df_stage.withColumn('invoice_date', to_date(col('invoice_date'), 'dd/MM/yyyy'))\n",
    "\n",
    "df_stage = df_stage \\\n",
    "    .withColumn('Ano', year('invoice_date')) \\\n",
    "    .withColumn('Mes', month('invoice_date')) \\\n",
    "    .withColumn('Dia', dayofmonth('invoice_date')) \\\n",
    "    .withColumn('Trimestre', quarter('invoice_date'))\n",
    "\n",
    "# df_stage.show()\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Campos decimais ou inteiros nulos ou vazios, deversão ser preenchidos por 0.\n",
    "\n",
    "cols_to_check = ['item_number', 'discount_amount', 'list_price', 'sales_amount', 'sales_amount_based_on_list_price', 'sales_cost_amount', 'sales_margin_amount', 'sales_price', 'line_number', 'sales_quantity']\n",
    "\n",
    "for col_name in cols_to_check:\n",
    "     df_stage = df_stage.withColumn(col_name, when(col(col_name) == '', 0).otherwise(col(col_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Campos strings vazios deverão ser preenchidos com 'Não informado'\n",
    "\n",
    "# cols_to_check = ['item_class', 'item', 'business_family_name', 'customer', 'customer_type', 'line_of_business', 'phone', 'regional_sales_mgr', 'search_type', 'region_name', 'division_name', 'city', 'country', 'customer_address_1', 'customer_address_2', 'customer_address_3', 'customer_address_4', 'state', 'zip_code']\n",
    "\n",
    "# for col_name in cols_to_check:\n",
    "#      df_stage = df_stage.withColumn(col_name, when(col(col_name) == 'null', \"Nao informado\").otherwise(col(col_name)))\n",
    "\n",
    "#Campos strings vazios deverão ser preenchidos com 'Não informado'\n",
    "all_columns = df_stage.columns\n",
    "\n",
    "for column in all_columns:\n",
    "   df_stage = df_stage.withColumn(column, \n",
    "                                  when(trim(regexp_replace(col(column), '\\n', 'null')) == \"\", \"Nao Informado\")\n",
    "                                  .otherwise(col(column)))\n",
    "\n",
    "#Campos strings nulos deverão ser preenchidos com 'Não informado'\n",
    "df_stage = df_stage.fillna(\"Nao Informado\")\n",
    "\n",
    "\n",
    "# #Filtro para verificar se determinado cliente que tinha strings vaziasforam substituídos com 'Não informado'\n",
    "# df_stage.filter((col(\"customer_key\") == \"10016588\") & (col(\"invoice_number\") == \"110753\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------+---------------+------------+--------------+-------------+-----------+------------------------------+-----------+----------+------------+----------------------+------------+--------------------------------+-----------------+-------------------+-----------+--------------+---------+---+--------------+--------------------+-------------+----------------+------------+-------------+--------+----------------+------------+-----------+------------------+-----------+-----------------------+-------+-------+---------------------+------------------+------------------+------------------+-----+----------+----------------+-------------+------------------+-----------+----+---+---+---------+\n",
      "|actual_delivery_date|customer_key_vendas|date_key  |discount_amount|invoice_date|invoice_number|item_class   |item_number|item                          |line_number|list_price|order_number|promised_delivery_date|sales_amount|sales_amount_based_on_list_price|sales_cost_amount|sales_margin_amount|sales_price|sales_quantity|sales_rep|u_m|address_number|business_family_name|business_unit|customer        |customer_key|customer_type|division|line_of_business|phone       |region_code|regional_sales_mgr|search_type|address_number_endereco|city   |country|customer_address_1   |customer_address_2|customer_address_3|customer_address_4|state|zip_code  |division_divisao|division_name|region_code_regiao|region_name|Ano |Mes|Dia|Trimestre|\n",
      "+--------------------+-------------------+----------+---------------+------------+--------------+-------------+-----------+------------------------------+-----------+----------+------------+----------------------+------------+--------------------------------+-----------------+-------------------+-----------+--------------+---------+---+--------------+--------------------+-------------+----------------+------------+-------------+--------+----------------+------------+-----------+------------------+-----------+-----------------------+-------+-------+---------------------+------------------+------------------+------------------+-----+----------+----------------+-------------+------------------+-----------+----+---+---+---------+\n",
      "|16/08/2019          |10016588           |16/08/2018|-698,4         |2018-08-18  |110753        |Nao Informado|64420      |Blue Label Canned String Beans|1000       |0         |208991      |16/08/2019            |698,4       |0                               |361,32           |337,08             |69,84      |10            |104      |EA |10016588      |R3                  |1            |Bounds Megaplace|10016588    |G2           |2       |M1              |816-455-8733|2          |S16               |C          |10016588               |Schertz|US     |6025 Corridor Parkway|Nao Informado     |Nao Informado     |Nao Informado     |TX   |78154-3214|2               |Domestic     |2                 |Southern   |2018|8  |18 |3        |\n",
      "|16/08/2019          |10016588           |16/08/2018|0              |2018-08-18  |110753        |Nao Informado|0          |Monarch Manicotti             |2000       |0         |208991      |16/08/2019            |401,86      |0                               |0                |401,86             |0          |0             |104      |EA |10016588      |R3                  |1            |Bounds Megaplace|10016588    |G2           |2       |M1              |816-455-8733|2          |S16               |C          |10016588               |Schertz|US     |6025 Corridor Parkway|Nao Informado     |Nao Informado     |Nao Informado     |TX   |78154-3214|2               |Domestic     |2                 |Southern   |2018|8  |18 |3        |\n",
      "+--------------------+-------------------+----------+---------------+------------+--------------+-------------+-----------+------------------------------+-----------+----------+------------+----------------------+------------+--------------------------------+-----------------+-------------------+-----------+--------------+---------+---+--------------+--------------------+-------------+----------------+------------+-------------+--------+----------------+------------+-----------+------------------+-----------+-----------------------+-------+-------+---------------------+------------------+------------------+------------------+-----+----------+----------------+-------------+------------------+-----------+----+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #Filtro para verificar se determinado cliente que tinha valores vazio foram substituidos por zero\n",
    "df_stage.filter((col(\"customer_key\") == \"10016588\") & (col(\"invoice_number\") == \"110753\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Campos strings vazios deverão ser preenchidos com 'Não informado'\n",
    "#all_columns = df_stage.columns\n",
    "\n",
    "#for column in all_columns:\n",
    "#    df_stage = df_stage.withColumn(column, \n",
    "#                                   when(trim(regexp_replace(col(column), '\\n', '')) == \"\", \"Nao informado\")\n",
    "#                                   .otherwise(col(column)))\n",
    "\n",
    "# #Filtro para verificar se determinado cliente que tinha strings vaziasforam substituídos com 'Não informado'\n",
    "# df_stage.filter((col(\"customer_key\") == \"10016588\") & (col(\"invoice_number\") == \"110753\")).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+----------+---------------+------------+--------------+----------+-----------+--------------------+-----------+----------+------------+----------------------+------------+--------------------------------+-----------------+-------------------+-----------+--------------+---------+---+--------------+--------------------+-------------+-----------+------------+-------------+--------+----------------+------------+-----------+------------------+-----------+-----------------------+-------------+-------------+------------------+------------------+------------------+------------------+-------------+-------------+----------------+-------------+------------------+-------------+----+---+---+---------+\n",
      "|actual_delivery_date|customer_key_vendas|  date_key|discount_amount|invoice_date|invoice_number|item_class|item_number|                item|line_number|list_price|order_number|promised_delivery_date|sales_amount|sales_amount_based_on_list_price|sales_cost_amount|sales_margin_amount|sales_price|sales_quantity|sales_rep|u_m|address_number|business_family_name|business_unit|   customer|customer_key|customer_type|division|line_of_business|       phone|region_code|regional_sales_mgr|search_type|address_number_endereco|         city|      country|customer_address_1|customer_address_2|customer_address_3|customer_address_4|        state|     zip_code|division_divisao|division_name|region_code_regiao|  region_name| Ano|Mes|Dia|Trimestre|\n",
      "+--------------------+-------------------+----------+---------------+------------+--------------+----------+-----------+--------------------+-----------+----------+------------+----------------------+------------+--------------------------------+-----------------+-------------------+-----------+--------------+---------+---+--------------+--------------------+-------------+-----------+------------+-------------+--------+----------------+------------+-----------+------------------+-----------+-----------------------+-------------+-------------+------------------+------------------+------------------+------------------+-------------+-------------+----------------+-------------+------------------+-------------+----+---+---+---------+\n",
      "|          10/02/2020|           10021270|10/02/2019|         200,86|  2019-02-12|        223285|       P01|      60448|High Top Summer S...|       2000|    493,85|      319358|            10/02/2020|      292,99|                          493,85|           207,23|              85,76|     292,99|             1|      109| EA|      10021270|                  R3|            1|Page Market|    10021270|           G2|       1|   Nao Informado|816-455-8733|          5|               S16|          C|          Nao Informado|Nao Informado|Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|Nao Informado|Nao Informado|               1|International|                 5|International|2019|  2| 12|        1|\n",
      "|          23/02/2020|           10021270|23/02/2019|        1746,32|  2019-02-25|        224270|       P01|      17801|Better Fancy Cann...|       5000|   1431,23|      319835|            23/02/2020|     2547,37|                         4293,69|          1349,08|            1198,29|849,1233333|             3|      109| EA|      10021270|                  R3|            1|Page Market|    10021270|           G2|       1|   Nao Informado|816-455-8733|          5|               S16|          C|          Nao Informado|Nao Informado|Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|Nao Informado|Nao Informado|               1|International|                 5|International|2019|  2| 25|        1|\n",
      "|          23/02/2020|           10021270|23/02/2019|         393,07|  2019-02-25|        224270|       P01|      28401|Ebony Prepared Salad|       8000|    966,44|      319835|            23/02/2020|      573,37|                          966,44|           278,38|             294,99|     573,37|             1|      109| EA|      10021270|                  R3|            1|Page Market|    10021270|           G2|       1|   Nao Informado|816-455-8733|          5|               S16|          C|          Nao Informado|Nao Informado|Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|Nao Informado|Nao Informado|               1|International|                 5|International|2019|  2| 25|        1|\n",
      "|          23/02/2020|           10021270|23/02/2019|         206,52|  2019-02-25|        224270|       P01|      29255|Thresher Spicy Mints|       9000|    507,75|      319835|            23/02/2020|      301,23|                          507,75|           162,89|             138,34|     301,23|             1|      109| EA|      10021270|                  R3|            1|Page Market|    10021270|           G2|       1|   Nao Informado|816-455-8733|          5|               S16|          C|          Nao Informado|Nao Informado|Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|Nao Informado|Nao Informado|               1|International|                 5|International|2019|  2| 25|        1|\n",
      "|          23/02/2020|           10021270|23/02/2019|         156,45|  2019-02-25|        224270|       P01|      11690|High Top Dried Mu...|       3000|    192,34|      319835|            23/02/2020|      228,23|                          384,68|            66,78|             161,45|    114,115|             2|      109| SE|      10021270|                  R3|            1|Page Market|    10021270|           G2|       1|   Nao Informado|816-455-8733|          5|               S16|          C|          Nao Informado|Nao Informado|Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|Nao Informado|Nao Informado|               1|International|                 5|International|2019|  2| 25|        1|\n",
      "|          29/02/2020|           10021270|01/03/2019|         156,93|  2019-03-03|        224737|       P01|      29460|BBB Best Chunky P...|      16000|    385,84|      319835|            29/02/2020|      228,91|                          385,84|            97,88|             131,03|     228,91|             1|      109| EA|      10021270|                  R3|            1|Page Market|    10021270|           G2|       1|   Nao Informado|816-455-8733|          5|               S16|          C|          Nao Informado|Nao Informado|Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|Nao Informado|Nao Informado|               1|International|                 5|International|2019|  3|  3|        1|\n",
      "|          14/03/2020|           10021270|14/03/2019|         786,14|  2019-03-17|        225968|       P01|      28401|Ebony Prepared Salad|       3000|    966,44|      320988|            14/03/2020|     1146,74|                         1932,88|           544,55|             602,19|     573,37|             2|      109| EA|      10021270|                  R3|            1|Page Market|    10021270|           G2|       1|   Nao Informado|816-455-8733|          5|               S16|          C|          Nao Informado|Nao Informado|Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|Nao Informado|Nao Informado|               1|International|                 5|International|2019|  3| 17|        1|\n",
      "|          14/03/2020|           10021270|14/03/2019|         312,26|  2019-03-17|        225968|       P01|      45880|Red Spade Low Fat...|       7000|    767,75|      320988|            14/03/2020|      455,49|                          767,75|           282,05|             173,44|     455,49|             1|      109| EA|      10021270|                  R3|            1|Page Market|    10021270|           G2|       1|   Nao Informado|816-455-8733|          5|               S16|          C|          Nao Informado|Nao Informado|Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|Nao Informado|Nao Informado|               1|International|                 5|International|2019|  3| 17|        1|\n",
      "|          14/03/2020|           10021270|14/03/2019|         671,06|  2019-03-17|        225968|       P01|      20910|  Moms Sliced Turkey|       2000|    824,96|      320988|            14/03/2020|      978,86|                         1649,92|           509,75|             469,11|     489,43|             2|      109| EA|      10021270|                  R3|            1|Page Market|    10021270|           G2|       1|   Nao Informado|816-455-8733|          5|               S16|          C|          Nao Informado|Nao Informado|Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|Nao Informado|Nao Informado|               1|International|                 5|International|2019|  3| 17|        1|\n",
      "|          14/03/2020|           10021270|14/03/2019|        1986,22|  2019-03-17|        225968|       P01|      47801|Red Spade Foot-Lo...|       8000|   1627,84|      320988|            14/03/2020|      2897,3|                         4883,52|          1427,24|            1470,06|965,7666667|             3|      109| EA|      10021270|                  R3|            1|Page Market|    10021270|           G2|       1|   Nao Informado|816-455-8733|          5|               S16|          C|          Nao Informado|Nao Informado|Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|     Nao Informado|Nao Informado|Nao Informado|               1|International|                 5|International|2019|  3| 17|        1|\n",
      "+--------------------+-------------------+----------+---------------+------------+--------------+----------+-----------+--------------------+-----------+----------+------------+----------------------+------------+--------------------------------+-----------------+-------------------+-----------+--------------+---------+---+--------------+--------------------+-------------+-----------+------------+-------------+--------+----------------+------------+-----------+------------------+-----------+-----------------------+-------------+-------------+------------------+------------------+------------------+------------------+-------------+-------------+----------------+-------------+------------------+-------------+----+---+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "df_stage.filter(col(\"address_number\") == \"10021270\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Conta o número de valores vazios em todas as colunas\n",
    "# empty_counts = [df_stage.select(when(col(c) == \"\", 1).otherwise(0)).agg({\"*\": \"sum\"}).collect()[0][0] for c in df_stage.columns]\n",
    "# for i, c in enumerate(df_stage.columns):\n",
    "#     print(f\"{c}: {empty_counts[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Verificar se há valores nulos em colunas numéricas\n",
    "# from pyspark.sql.functions import isnan, col\n",
    "# from pyspark.sql.types import DoubleType, IntegerType, LongType, FloatType\n",
    "\n",
    "# numeric_cols = [c.name for c in df_stage.schema if c.dataType in [DoubleType(), IntegerType(), LongType(), FloatType()]]\n",
    "\n",
    "# null_counts = [df_stage.select(isnan(col(c)).alias(c)).agg({\"*\": \"sum\"}).collect()[0][0] for c in numeric_cols]\n",
    "# for i, c in enumerate(numeric_cols):\n",
    "#     print(f\"{c}: {null_counts[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicionar chaves estrangeiras\n",
    "df_stage = df_stage.withColumn('PK_TEMPO', sha2(concat_ws(\"\",df_stage.invoice_date, df_stage.Ano,df_stage.Mes,df_stage.Dia,df_stage.Trimestre), 256))\n",
    "\n",
    "df_stage = df_stage.withColumn('PK_CLIENTES', sha2(concat_ws(\"\",df_stage.customer_key,df_stage.customer,df_stage.business_family_name,df_stage.business_unit,df_stage.customer_type,df_stage.division,df_stage.line_of_business,df_stage.phone,df_stage.region_code,df_stage.regional_sales_mgr,df_stage.search_type), 256))\n",
    "\n",
    "df_stage = df_stage.withColumn('PK_LOCALIDADE', sha2(concat_ws(\"\",df_stage.address_number,df_stage.city,df_stage.country,df_stage.state,df_stage.zip_code,df_stage.division,df_stage.division_name,df_stage.region_code,df_stage.region_name,df_stage.customer_address_1,df_stage.customer_address_2,df_stage.customer_address_3,df_stage.customer_address_4), 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_stage.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage.createOrReplaceTempView(\"stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerar dimensões e fato a partir do tabelão  \n",
    "df_tempo = spark.sql(\"SELECT DISTINCT PK_TEMPO, invoice_date, Ano, Mes, Dia, Trimestre FROM stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs dfs -get /datalake/gold/dim_tempo/part-* input/projeto-hive/gold/dim_tempo.csv\n"
     ]
    }
   ],
   "source": [
    "#Exportar dataframes como tabelas csv\n",
    "salvar_df(df_tempo, 'dim_tempo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clientes = spark.sql(\"SELECT DISTINCT PK_CLIENTES, customer_key, customer, business_family_name, business_unit, customer_type, division, line_of_business, phone, region_code, regional_sales_mgr, search_type FROM stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs dfs -get /datalake/gold/dim_clientes/part-* input/projeto-hive/gold/dim_clientes.csv\n"
     ]
    }
   ],
   "source": [
    "salvar_df(df_clientes, 'dim_clientes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_localidade = spark.sql(\"SELECT DISTINCT PK_LOCALIDADE, address_number, city, country, state, zip_code, division, division_name, region_code, region_name, customer_address_1, customer_address_2, customer_address_3, customer_address_4 FROM stage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs dfs -get /datalake/gold/dim_localidade/part-* input/projeto-hive/gold/dim_localidade.csv\n"
     ]
    }
   ],
   "source": [
    "salvar_df(df_localidade, 'dim_localidade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_vendas = spark.sql(\"SELECT PK_CLIENTES, PK_TEMPO, PK_LOCALIDADE, actual_delivery_date, date_key, discount_amount, invoice_date, invoice_number, item_class, item_number, item, line_number, list_price, order_number, promised_delivery_date, sales_amount, sales_amount_based_on_list_price, sales_cost_amount, sales_margin_amount, sales_price, sales_quantity, sales_rep, u_m  FROM stage\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs dfs -get /datalake/gold/ft_vendas/part-* input/projeto-hive/gold/ft_vendas.csv\n"
     ]
    }
   ],
   "source": [
    "salvar_df(ft_vendas, 'ft_vendas')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
