{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, dataframe\n",
    "from pyspark.sql.functions import when, col, sum, count, isnan, round\n",
    "from pyspark.sql.functions import regexp_replace, concat_ws, sha2, rtrim, substring\n",
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, to_date\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
    "from pyspark.sql import HiveContext\n",
    "from pyspark.sql.functions import year, month, dayofmonth, quarter\n",
    "\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salvar_df(df, file):\n",
    "    output = \"/input/projeto_hive/gold/\" + file\n",
    "    erase = \"hdfs dfs -rm \" + output + \"/*\"\n",
    "    rename = \"hdfs dfs -get /datalake/gold/\"+file+\"/part-* /input/projeto_hive/gold/\"+file+\".csv\"\n",
    "    print(rename)\n",
    "    \n",
    "    \n",
    "    df.coalesce(1).write\\\n",
    "        .format(\"csv\")\\\n",
    "        .option(\"header\", True)\\\n",
    "        .option(\"delimiter\", \";\")\\\n",
    "        .mode(\"overwrite\")\\\n",
    "        .save(\"/datalake/gold/\"+file+\"/\")\n",
    "\n",
    "    os.system(erase)\n",
    "    os.system(rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clientes = spark.sql(\"select * from desafio_curso.clientes\")\n",
    "df_divisao = spark.sql(\"select * from desafio_curso.divisao\")\n",
    "df_endereco = spark.sql(\"select * from desafio_curso.endereco\")\n",
    "df_regiao = spark.sql(\"select * from desafio_curso.regiao\")\n",
    "df_vendas = spark.sql(\"select * from desafio_curso.vendas\")\n",
    "\n",
    "df_endereco.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazer um tabelão\n",
    "\n",
    "df_stage_clientes_localidade = df_clientes.join(df_endereco, \"address_number\", \"inner\")\\\n",
    "                                .join(df_divisao, \"division\", \"inner\")\\\n",
    "                                .join(df_regiao, \"region_code\", \"inner\")\\\n",
    "                                .join(df_vendas, \"customer_key\", \"inner\")\n",
    "\n",
    "df_stage_clientes_localidade.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adicionar colunas Ano, Mês, Dia, Trimestre tomando como base a coluna invoice_date\n",
    "\n",
    "df_stage_clientes_localidade = df_stage_clientes_localidade.withColumn('invoice_date', to_date(col('invoice_date'), 'dd/MM/yyyy'))\n",
    "\n",
    "df_stage = df_stage_clientes_localidade \\\n",
    "    .withColumn('Ano', year('invoice_date')) \\\n",
    "    .withColumn('Mês', month('invoice_date')) \\\n",
    "    .withColumn('Dia', dayofmonth('invoice_date')) \\\n",
    "    .withColumn('Trimestre', quarter('invoice_date'))\n",
    "\n",
    "df_stage.show() \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Campos strings vazios deverão ser preenchidos com 'Não informado'\n",
    "\n",
    "for column in df_stage.columns:\n",
    "    if df_stage.schema[column].dataType in (DecimalType(), IntegerType()):\n",
    "        df_stage = df_stage.withColumn(column, when(col(column).isNull() | (col(column) == \"\"), 0).otherwise(col(column)))\n",
    "    else:\n",
    "        df_stage = df_stage.withColumn(column, when(col(column) == \"\", \"Não informado\").otherwise(col(column)))\n",
    "        \n",
    "df_stage.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stage.filter(col(\"address_number\") == \"10000453\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
